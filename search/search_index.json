{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FICTURE documentation","text":"<p>Documentation overview</p> <p>Run: how to run FICTURE for a large dataset in a linux environment including how to submit jobs to SLURM.</p> <p>Format: how to prepare the input files from different platforms' raw output.</p>"},{"location":"format_input/","title":"Format raw output from different platforms","text":"<p>FICTURE only needs a file with 2D coordinates, gene/transcript ID, and the corresponding transcript count.</p> <p>We have tested FICTURE on Seq-scope, Stereo-seq, CosMx SMI, Xenium, MERSCOPE, and Visium HD data. Here we document how to format the raw data from each platform to the required input files like the following example data.</p> <pre><code>examples/data/transcripts.tsv.gz\nexamples/data/feature.clean.tsv.gz\n</code></pre> <p>CosMx SMI</p> <p>Visium HD</p>"},{"location":"run/","title":"Real data process","text":"<p>We take a small sub-region of Vizgen MERSCOPE mouse liver data as an example. (The same region we showed in supplementary figure X)</p>"},{"location":"run/#input","title":"Input","text":"<p><pre><code>examples/data/transcripts.tsv.gz\nexamples/data/feature.clean.tsv.gz\n</code></pre> (All filese are tab-delimited text files unless specified otherwise.)</p> <p>Transcripts</p> <p>One file contains the molecular or pixel level information, the required columns are <code>X</code>, <code>Y</code>, <code>gene</code>, and <code>Count</code>. (There could be other columns in the file which would be ignored.)</p> <p>The coordinates <code>(X, Y)</code> can be float or integer numbers in arbitrary units, but if it is not in the unit of \\(\\mu m\\) we would need to specify the translation ratio later.</p> <p>The file has to be sorted by one of the coordinates. (Usually it is the longer axis, but it does not matter if the tissue area is not super asymmetric.)</p> <p><code>Count</code> (could be any other name) is the number of transcripts for the specified <code>gene</code> observed at the coordinate. For imaging based technologies where each molecule has its unique coordinates, <code>Count</code> could be always 1.</p> <p>Gene list</p> <p>Another file contains the (unique) names of genes that should be used in analysis. The required columns is just <code>gene</code> (including the header), the naming of genes should match the <code>gene</code> column in the transcript file. If your data contain negative control probes or if you would like to remove certain genes this is where you can specify. (If you would like to use all genes present in your input transcript file the gene list is not necessary, but you would need to modify the command in <code>generic_III.sh</code> to remove the argument <code>--feature</code> )</p> <p>Meta data</p> <p>We also prefer to keep a file listing the min and max of the coordinates (this is primarily for visuaizing very big tissue region where we do not read all data at once but would want to know the image dimension). The unit of the coordinates is micrometer. <pre><code>examples/data/coordinate_minmax.tsv\n</code></pre></p>"},{"location":"run/#process","title":"Process","text":"<p>Specify the base directory that contains the input data <pre><code>path=examples/data\n</code></pre></p> <p>Data specific setup:</p> <p><code>mu_scale</code> is the ratio between \\(\\mu m\\) and the unit used in the transcript coordinates. For example, if the coordinates are sotred in <code>nm</code> this number should be <code>1000</code>.</p> <p><code>key</code> is the column name in the transcripts file corresponding to the gene counts (<code>Count</code> in our example). <code>MJ</code> specify which axis the transcript file is sorted by.</p> <pre><code>mu_scale=1 # If your data's coordinates are already in micrometer\nkey=Count\nMJ=Y # If your data is sorted by the Y-axis\nenv=venv/with/ficture/installed/bin/activate\ngitpath=path/to/ficture # path to this repository\n#SLURM_ACCOUNT= # For submitting jobs to slurm\n</code></pre> <p>Example bash scripts are in <code>examples/script/</code>, you will need to modify them to work on your system.</p> <p>Create pixel minibatches (<code>${path}/batched.matrix.tsv.gz</code>) <pre><code>input=${path}/transcripts.tsv.gz\noutput=${path}/batched.matrix.tsv.gz\nrec=$(sbatch --job-name=vz1 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=1 examples/script/generic_I.sh input=${input} output=${output} MJ=${MJ} env=${env} gitpath=${gitpath})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid1=${ADDR[3]}\n</code></pre></p> <p>Parameters for initializing the model <pre><code>nFactor=12 # Number of factors\nsliding_step=2\ntrain_nEpoch=3\ntrain_width=12 # \\sqrt{3} x the side length of the hexagon (um)\nmodel_id=nF${nFactor}.d_${train_width} # An identifier kept in output file names\nmin_ct_per_feature=20 # Ignore genes with total count \\&lt; 20\nR=10 # We use R random initializations and pick one to fit the full model\nthread=4 # Number of threads to use\nfeature=${path}/feature.clean.tsv.gz\n</code></pre></p> <p>Parameters for pixel level decoding <pre><code>fit_width=12 # Often equal or smaller than train_width (um)\nanchor_res=4 # Distance between adjacent anchor points (um)\nradius=$(($anchor_res+1))\nanchor_info=prj_${fit_width}.r_${anchor_res} # An identifier\ncoor=${path}/coordinate_minmax.tsv\n</code></pre></p> <p>Model fitting <pre><code># Prepare training minibatches, only need to run once if you plan to fit multiple models (say with different number of factors)\ninput=${path}/transcripts.tsv.gz\nhexagon=${path}/hexagon.d_${train_width}.tsv.gz\nrec=$(sbatch --job-name=vz2 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=1 examples/script/generic_II.sh env=${env} gitpath=${gitpath} key=${key} mu_scale=${mu_scale} major_axis=${MJ} path=${path} input=${input} output=${hexagon} width=${train_width} sliding_step=${sliding_step})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid2=${ADDR[3]}\n\n# Model training\nrec=$(sbatch --job-name=vz3 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=${thread} --dependency=afterok:${jobid2} examples/script/generic_III.sh env=${env} gitpath=${gitpath} key=${key} mu_scale=${mu_scale} major_axis=${MJ} path=${path} pixel=${input} hexagon=${hexagon} feature=${feature} model_id=${model_id} train_width=${train_width} nFactor=${nFactor} R=${R} train_nEpoch=${train_nEpoch} fit_width=${fit_width} anchor_res=${anchor_res} min_ct_per_feature=${min_ct_per_feature} thread=${thread})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid3=${ADDR[3]}\n\n# Pixel level decoding &amp; visualization\nrec=$(sbatch --job-name=vz4 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=${thread} --dependency=afterok:${jobid3},${jobid1} examples/script/generic_V.sh env=${env} gitpath=${gitpath} key=${key} mu_scale=${mu_scale} path=${path} model_id=${model_id} anchor_info=${anchor_info} radius=${radius} coor=${coor} thread=${thread})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid4=${ADDR[3]}\n</code></pre></p>"},{"location":"run/#output","title":"Output","text":"<p>In the above example the analysis outputs are stored in <pre><code>${path}/analysis/${model_id} # examples/data/analysis/nF12.d_12\n</code></pre></p> <p>There is an html file reporting the color code and top genes of the inferred factors <pre><code>nF12.d_12.decode.prj_12.r_4_5.factor.info.html\n</code></pre></p> <p>Pixel level visualizating <pre><code>figure/nF12.d_12.decode.prj_12.r_4_5.pixel.png\n</code></pre></p> <p>Pixel level output is <pre><code>nF12.d_12.decode.prj_12.r_4_5.pixel.sorted.tsv.gz\n</code></pre></p> <p>We store the top 3 factors and their corresponding posterior probabilities for each pixel in tab delimted text files. As a temporary hack for accessing specific regions in large dataset faster, we divided the data along one axis (X or Y), sorted within each block by the other axis. The first 3 lines of the file, starting with <code>##</code>, are metadata, the 4<sup>th</sup> line, starting with <code>#</code>, contains columns names. To use the file as plain text, you can ignore this complication and read the file from the 4<sup>th</sup> line.</p> <p>The first few lines of the file are as follows:</p> <pre><code>##K=12;TOPK=3\n##BLOCK_SIZE=2000;BLOCK_AXIS=X;INDEX_AXIS=Y\n##OFFSET_X=6690;OFFSET_Y=6772;SIZE_X=676;SIZE_Y=676;SCALE=100\n#BLOCK  X       Y       K1      K2      K3      P1      P2      P3\n0       10400   360     2       1       8       9.07e-01        9.27e-02        2.61e-13\n0       10669   360     2       1       8       9.36e-01        6.37e-02        4.20e-08\n0       10730   360     2       1       8       8.85e-01        1.15e-01        1.83e-05\n</code></pre> <p>The 4<sup>th</sup> line contains the column names. From the 5<sup>th</sup> line on, each line contains the information for one pixel with coordinates <code>(X, Y)</code>, the top 3 factors indicated by <code>K1, K2, K3</code> and their corresponding posterior probabilities <code>P1, P2, P3</code>. Factors are 0-indexed.</p> <p>The 1<sup>st</sup> line indicates that the data is from a model with 12 factors (<code>K=12</code>) and we store the top 3 factors for each pixel (<code>TOPK=3</code>).</p> <p>The 2<sup>nd</sup> line indicates that the data is separated into blocks by the X axis (<code>BLOCK_AXIS=X</code>) with block size 2000\\(\\mu m\\) (<code>BLOCK_SIZE=2000</code>), then within each block the data is sorted by the Y axis (<code>INDEX_AXIS=Y</code>). The block IDs (first column in the file) are integer multiples of the block size (in \\(\\mu m\\)), i.e. the 1<sup>st</sup> block, with \\(X \\in [0, 2000)\\) have block ID 0, the 2<sup>nd</sup> block, with \\(X \\in [2000, 4000)\\) have block ID 2000, etc.</p> <p>The 3<sup>rd</sup> line describes the translation between the stored cooredinates and the physical coordinates in \\(\\mu m\\). Take <code>(X, Y)</code> as a pixel coordinates read from the file, the physical coordinates in \\(\\mu m\\) is <code>(X / SCALE + OFFSET_X, Y / SCALE + OFFSET_Y)</code>. In this above example, the raw data from Vizgen MERSCOPE mouse liver data contains negative coordinates, but for convineince we shifted all coordinates to positive. <code>SIZE_X</code> and <code>SIZE_Y</code> record the size of the raw data in \\(\\mu m\\).</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"format_input/cosmx/","title":"Process CosMx SMI raw data","text":"<p>Locate the transcript file from your SMI output, mostly likely it is named <code>*_tx_file.csv.gz</code> with the following columns</p> <pre><code>\"fov\",\"cell_ID\",\"x_global_px\",\"y_global_px\",\"x_local_px\",\"y_local_px\",\"z\",\"target\",\"CellComp\"\n1,0,298943.990047619,19493.2809095238,896.371,4433.7571,0,\"Snap25\",\"None\"\n1,0,298685.619047619,19489.3238095238,638,4429.8,0,\"Fth1\",\"None\"\n1,0,298688.648047619,19487.8095095238,641.029,4428.2857,0,\"Dnm1\",\"None\"\n1,0,298943.890047619,19478.7667095238,896.271,4419.2429,0,\"Pbx1\",\"Nuclear\"\n</code></pre> <p>What we need are <code>x_global_px</code>, <code>y_global_px</code>, and <code>target</code>.</p> <p>We would also like to translate the pixel unit into micrometer, the ratio can be found in a SMI Data File ReadMe come with your raw data. For example, for the public mouse brain dataset the README says</p> <ul> <li>x_local_px<ul> <li>The x position of this transcript within the FOV, measured in pixels. To convert to microns multiply the pixel value by 0.168 um per pixel.</li> </ul> </li> </ul> <p>So in the following commands we set <code>px_to_um=0.168</code>.</p> <p>The python script can be found in <code>ficture/misc</code>.</p> <pre><code>input=/path/to/input/Tissue5_tx_file.csv.gz # Change it to your transcript file\npath=/path/to/output\niden=brain # how you identify your files\ndummy=NegPrb # Name of the negative controls\npx_to_um=0.168 # convert the pixel unit in the input to micrometer\n\noutput=${path}/filtered.matrix.${iden}.tsv\nfeature=${path}/feature.clean.${iden}.tsv.gz\n\npython format_cosmx.py --input ${input} --output ${output} --feature ${feature} --dummy_genes ${dummy} --px_to_um ${px_to_um}\nsort -k2,2g -k1,1g ${output} | gzip -c &gt; ${output}.gz\nrm ${output}\n</code></pre>"},{"location":"format_input/visiumHD/","title":"Process Visium HD raw data","text":"<p>Visium HD output contains a sparse count matrix and a separate file defining pixels' spatial locations. The following is how to combine these two pieces of information with command lines.</p> <p>Visium HD outputs <pre><code>brc_pos=/path/to/tissue_positions.csv\nmtx_path=/path/to/count/matrix/filtered_feature_bc_matrix\nopath=/output/directory\n</code></pre></p> <p>The <code>brc_pos</code>, (<code>tissue_positions.csv</code>) file looks like <pre><code>barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres\nCGAGGATATTCAGAGC-1,0,0,0,45641,6872\nTCTGGTACTAATGCGG-1,0,0,2,45641,7238\n</code></pre></p> <p>The matrix directory looks like <pre><code>ls ${mtx_path}\n</code></pre></p> <pre><code>barcodes.tsv.gz  features.tsv.gz  matrix.mtx.gz\n</code></pre> <p>The following command does the following steps 1 Match the barcode indices in the sparse matrix (<code>matrix.mtx.gz</code>) with their global spatial locations, here would be the 5<sup>th</sup> and 6<sup>th</sup> columns, <code>pxl_row_in_fullres,pxl_col_in_fullres</code>, in <code>tissue_positions.csv</code> 2 Match gene ids with gene indices (this is trivial) 3 Combine all the information into one file, sort by coordinate on one axis (here is the X-axis).</p> <p><pre><code>bfile=${mtx_path}/barcodes.tsv.gz\nmfile=${mtx_path}/matrix.mtx.gz\nffile=${mtx_path}/features.tsv.gz\n\noutput=${opath}/transcripts.tsv.gz\n\nawk 'BEGIN{FS=OFS=\"\\t\"} NR==FNR{ft[NR]=$1 FS $2; next} ($5 in ft) {print $1 FS $3 FS $4 FS ft[$5] FS $6 }' &lt;(zcat $ffile) &lt;(join -t $'\\t' -1 4 -2 2 &lt;(join -t $'\\t' -1 1 -2 1 &lt;(cut -d',' -f 1,5,6 ${brc_pos} | sed 's/,/\\t/g' | sort -k1,1 ) &lt;(zcat ${mtx_path}/barcodes.tsv.gz | cat -n | tr -s ' ' | awk ' {print $2 \"\\t\" $1} ' ) ) &lt;(zcat $mfile | tail -n +4 | sed 's/ /\\t/g') ) | sort -S 1G -k2,2n -k3,3n | sed '1 s/^/#barcode_idx\\tX\\tY\\tgene_id\\tgene\\tCount\\n/' | gzip -c &gt; ${output}\n</code></pre> (The sorting by coordinates part can take some time for large data, you could check the unsorted output from the above command up to right before <code>sort -S 1G -k2,2n -k3,3n</code>  first to see if it makes sense)</p> <p>Output looks like <pre><code>zcat $output | head\n</code></pre></p> <p><pre><code>#barcode_idx    X       Y       gene_id gene    Count\n1       13089   15433   ENSMUSG00000000001      Gnai3   13\n1       13089   15433   ENSMUSG00000000028      Cdc45   8\n1       13089   15433   ENSMUSG00000000056      Narf    12\n1       13089   15433   ENSMUSG00000000058      Cav2    13\n</code></pre> (The first column is just to retain the original barcode index in <code>matrix.mtx.gz</code>, it will be ignored in analysis)</p>"}]}